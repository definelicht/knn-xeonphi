\section{$k$-Nearest Neighbor Search Algorithm}
  \label{sec:_k_nearest_neighbor_search_algorithm}

  In this section we formally define the $k$-nearest neighbor search problem 
  and introduce the algorithm to build a randomized k-d tree.

  \mypar{Nearest neighbor search} Consider a set of points $\mathcal{P} 
  = \{p_1,p_2,\dots,p_n\}$ in a metric space $\mathbb{M}$ which defines some 
  distance function $d\colon\mathbb{M}\times\mathbb{M}\to\mathbb{R}$.  If we 
  are given a query point $q\in\mathcal{P}$, the nearest neighbor of $q$ must 
  satisfy the condition
  \begin{align}
    \label{eq:NN}
    \operatorname{NN}(q,\mathcal{P}) &= \operatorname{argmin}_{x\in\mathcal{P}} 
    d(q,x)\in\mathcal{P}.
  \end{align}

  Often we are not interested in only one nearest neighbor, but the $k$~nearest 
  neighbors, for which we use the notation
  \begin{align}
    \label{eq:kNN}
    \operatorname{kNN}(q,\mathcal{P},k) &= \mathcal{A},
  \end{align}
  where $\mathcal{A}\subseteq\mathcal{P}$ with cardinality 
  $\vert\mathcal{A}\vert = k$.  The following constraint must further be 
  satisfied
  \begin{align}
    \label{eq:constraint_kNN}
    \{d(q,x)\mid d(q,x)\leq d(q,y)\forall x\in\mathcal{A}, 
    y\in\mathcal{P}-\mathcal{A},q\in\mathcal{P}\}.
  \end{align}
  Since the cardinality of $\mathcal{A}$ is $k$, it follows that $n\geq k$ for 
  the number of points in $\mathcal{P}$.

  \mypar{Randomized k-d~tree build} Building the tree is similar to a regular 
  k-d tree~\cite{bentley1975a,friedman1977a} with the exception that the 
  randomized k-d tree splits the data in a $d$-dimensional metric space along 
  dimension $i_d$, which is chosen randomly out of $D$ possibilities, where $D$ 
  is the number of the data dimensions with highest variance.  Each point 
  $p\in\mathcal{P}$ for which the value of the $i_d$-th dimension is lower than 
  the value of the same dimension of the reference point at the current node is 
  placed in a reduced set of points $\mathcal{P}_l\subset\mathcal{P}$ or 
  $\mathcal{P}_r\subset\mathcal{P}$ otherwise, where 
  $\mathcal{P}_l\cup\mathcal{P}_r = \mathcal{P}$.
  The procedure is then recursively applied to the two new branches using the 
  corresponding set of points $\mathcal{P}=\mathcal{P}_l$ or 
  $\mathcal{P}=\mathcal{P}_r$ with a new random choice for the split dimension 
  $i_d$.  For the work presented here, we follow~\cite{muja2009a} and set 
  $D=5$.
  
  \mypar{Randomized k-d~tree search}
Multiple randomized k-d trees are searched for  $k$ nearest neighbors. For each query $q_i$ two bounded heaps $H_{1i}$ and $H_{2i}$ are maintained across the randomized k-d forest. $H_{1i}$ contains the $k$ closest neighbors and $H_{2i}$ maintains the most promising branches not yet searched across all trees. After examining a given maximum number of leaves over all the trees the search is stopped resulting in an approximation of the obtained $k$ nearest neighbors. The search can be parallelized trivially. 
  % Give a short, self-contained summary of necessary
  % background information. For example, assume you present an
  % implementation of sorting algorithms. You could organize into sorting
  % definition, algorithms considered, and asymptotic runtime statements. The goal of the
  % background section is to make the paper self-contained for an audience
  % as large as possible. As in every section
  % you start with a very brief overview of the section. Here it could be as follows: In this section 
  % we formally define the sorting problem we consider and introduce the algorithms we use
  % including a cost analysis.

  % \mypar{Sorting}
  % Precisely define sorting problem you consider.

  % \mypar{Sorting algorithms}
  % Explain the algorithm you use including their costs.

  % As an aside, don't talk about "the complexity of the algorithm.'' It's incorrect,
  % problems have a complexity, not algorithms.

  % mainfile: ./../report.tex
